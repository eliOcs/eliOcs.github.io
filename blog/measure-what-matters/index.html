<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <link rel="icon" href="/favicon.ico" sizes="32x32" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link
      href="https://fonts.googleapis.com/css2?family=JetBrains+Mono:wght@600;700&family=Literata:wght@300;400&display=swap"
      rel="stylesheet"
    />
    <link rel="stylesheet" href="/style.css" />
    <title>Blog > Measure what matters</title>
    <meta name="description" content="How to identify your team's performance issues? Five years ago I started tracking metrics for my team's performance at Filestage. Learn about satisfaction, PRs merged, technical tickets, and impact metrics." />
    <meta name="author" content="Elio Capella Sánchez" />
    
    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article" />
    <meta property="og:url" content="https://eliocapella.com/blog/measure-what-matters/" />
    <meta property="og:title" content="Measure what matters" />
    <meta property="og:description" content="How to identify your team's performance issues? Five years ago I started tracking metrics for my team's performance at Filestage. Learn about satisfaction, PRs merged, technical tickets, and impact metrics." />
    <meta property="og:image" content="https://eliocapella.com/blog/measure-what-matters/all-hands-metrics.png" />
    
    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image" />
    <meta property="twitter:url" content="https://eliocapella.com/blog/measure-what-matters/" />
    <meta property="twitter:title" content="Measure what matters" />
    <meta property="twitter:description" content="How to identify your team's performance issues? Five years ago I started tracking metrics for my team's performance at Filestage. Learn about satisfaction, PRs merged, technical tickets, and impact metrics." />
    <meta property="twitter:image" content="https://eliocapella.com/blog/measure-what-matters/all-hands-metrics.png" />
  </head>
  <body>
    <div class="page-header">
      <a href="/" class="home-nav">
        <img src="/selfie.jpg" alt="Go to home page" />
      </a>
      <h1>Measure what matters</h1>
    </div>
    <p>
      You can measure RAM and CPU usage or the latency of the database per query
      to find bottlenecks in your code, but how do you identify your
      <em>team's</em>
      performance issues?
    </p>
    <p>
      Five years ago I started tracking metrics for my team's performance at
      Filestage. Measuring development productivity has always been
      controversial. Is it really better to more lines of code? We've all heard
      stories of massive performance improvements when
      <a
        href="https://www.linux-magazine.com/Online/News/Linux-Sees-Massive-Performance-Increase-from-a-Single-Line-of-Code"
        target="_blank"
        >changing a single line of code</a
      >. And by the way, <b>fast isn't enough</b> if your app is full of bugs or
      if customers don't understand how to use it...
      <em>are we actually delivering any value?</em>
    </p>
    <p>
      Before we lose our minds, we have to understand that
      <b
        >metrics will never be 100% accurate representations of our
        objectives</b
      >; they can always be gamed and should be used only as tools to quickly
      verify the state. If you really want to draw conclusions, you will have to
      dig deeper and actually talk to you team. This doesn't mean they are
      useless, but they must not cloud our judgment, and our focus should always
      be on our objectives.
    </p>
    <p>
      While we're at it, let's also clear another thing up:
      <b>not every metric has to be quantitative</b>. Interviews and surveys
      will give you valuable insights,
      <em>if your team is small that's often the most efficient approach</em>.
    </p>
    <p>
      So what are the objectives of a software development team?
      <b>Delivering on time</b> and <b>maintaining high quality</b>. If the
      software is delivered on time and has no bugs, no one will complain of
      engineering.
    </p>
    <p>
      So what is out there? When I started tracking, everyone was talking about
      <a href="https://queue.acm.org/detail.cfm?id=3182626" target="_blank"
        >DORA metrics and DevOps</a
      >; now it's about
      <a href="https://queue.acm.org/detail.cfm?id=3595878" target="_blank"
        >DevEx</a
      >
      and
      <a href="https://queue.acm.org/detail.cfm?id=3454124" target="_blank"
        >SPACE</a
      >
      , or is it the
      <a
        href="https://www.lennysnewsletter.com/p/introducing-core-4-the-best-way-to"
        target="_blank"
        >Core 4</a
      >? It's easy to get lost among so many frameworks, and if it's your first
      time approaching them they can seem too abstract. I'll share what has
      worked for me.
    </p>

    <h2>Satisfaction</h2>
    <p>
      I'm a firm believer that the
      <b>best work is done when people are happy</b> and satisfied. Yes, you can
      squeeze performance with pressure in the short term, but eventually people
      will burn out and leave. <em>Always play the long game</em>. I'm proud
      that my team consistently scores over 9/10 in satisfaction, even after we
      had to lay off 20% of the team to avoid bankruptcy at one point.
    </p>
    <p>
      I insist on putting this metric first because it's the foundation of a
      high-performing team. If you have very low satisfaction or participation,
      start digging deeper. Talk to your team; if you genuinely care about them
      you will gain their trust, and eventually they will share their biggest
      problems. Is your team drowning in technical debt and firefighting? Has
      your product team committed to unrealistic expectations, and the pressure
      is too high? Are team members ever recognized for their work?
    </p>
    <p>
      In our case, the People department sends surveys every two weeks, and we
      receive reports per department using the tool
      <a href="https://workleap.com/officevibe" target="_blank">Officevibe</a>.
      Every month I copy the satisfaction and participation values into a
      tracking Google Sheet.
    </p>
    <figure>
      <img
        src="./satisfaction-officevibe.png"
        alt="Officevibe satisfaction report"
      />
      <figcaption>Officevibe satisfaction report</figcaption>
    </figure>

    <h2>PRs Merged</h2>
    <p>
      This is the best measure of speed I've found so far. In our setup, every
      PR that is merged into master is released to production. Each PR must be
      approved by at least one other team member, and reviewers are assigned
      automatically. All automated CI checks—tests, linting, etc. must pass
      before a PR can be merged. Therefore
      <b>every PR represents a production-ready, completed technical task.</b
      ><br />
      Yes, like every other metric you could game this one by making absurdly
      small PRs, but what would the rest of the team think? Remember, PRs have
      to be reviewed.
      <em>It will encourage small PRs, but if you ask me that's a good thing</em
      >.
    </p>
    <p>
      On average, industry benchmarks show you should merge about
      <b>3 PRs per engineer each week</b> (elite teams reach up to 5). If you're
      below that, talk to your engineers and understand their biggest pain
      points. Are the tests taking too long, or are they too flaky? Is it simply
      taking a long time for PRs to be reviewed? After fixing the problems you
      can track your progress and see whether your improvements paid off—this is
      what it's all about: having a way to measure progress.
    </p>
    <p>
      In practice, this metric is easy to obtain: I query the GitHub API with a
      simple script that counts all PRs merged in the last 30 days and writes
      the number to a Google Sheet.
    </p>
    <figure>
      <img src="./merged-prs-sheet.png" alt="Merged PRs metric sheet" />
      <figcaption>Merged PRs metric sheet</figcaption>
    </figure>

    <h2>Technical Tickets</h2>
    <p>
      Moving fast is only beneficial if we can deliver with high quality. I like
      measuring the number of technical tickets; every ticket comes from a
      customer who actively uses our product and has been bothered enough to
      contact support about an issue. Most frameworks recommend using
      change-failure rate, but for us it is very rare to need immediate
      remediation after a deployment, so that metric isn't meaningful.
      <b>Don't blindly follow frameworks</b>, Always choose what is best for
      your current situation .
    </p>
    <figure>
      <img src="./asana-bug-board.png" alt="Asana technical ticket board" />
      <figcaption>Asana technical ticket board</figcaption>
    </figure>
    <p>
      Our company uses Asana, so I use their API to query the technical-tickets
      board and measure how many new tickets have been opened in the last month.
      As with other metrics, the script pushes this number to the Google Sheet
      every week.
    </p>

    <h2>Impact</h2>
    <p>
      In software development, not all of our work directly impacts the
      customer. Many times we refactor code or update a library to maintain our
      codebase, but the product doesn't improve for customers. In an ideal world
      we would spend 100% of our time working on new features. Benchmarks are
      interesting here: <b>elite teams spend 65% on new features</b>, while
      average teams spend about 59%.
    </p>
    <p>
      To gauge our team's impact, we require PR titles to follow the
      <a href="https://www.conventionalcommits.org/" target="_blank"
        >Conventional Commits spec</a
      >
      and count all PRs in the last month that were <code>feat</code>. This
      isn't perfect because working on new features isn't only about coding; it
      also involves talking with Product, understanding the concept, proposing a
      technical solution, researching, etc., whereas creating a bug fix is
      mostly coding.
    </p>
    <figure>
      <img
        src="./conventional-commits.png"
        alt="conventional commits in the git log"
      />
      <figcaption>conventional commits in the git log</figcaption>
    </figure>

    <h2>Conclusion</h2>
    <p>
      Comparing your team with others out there will help you
      <b>identify the biggest levers</b> for improving your team's performance.
      Most importantly, it will let you know when you're reaching a point of
      diminishing returns and when it's time to switch focus.
    </p>
    <figure>
      <img
        src="./all-hands-metrics.png"
        alt="Slide presenting metrics to the rest of the team"
      />
      <figcaption>Slide presenting metrics to the rest of the team</figcaption>
    </figure>
    <p>
      Use metrics to communicate the state of the team to the rest of the
      company; it's easier to tell a story if you can measure and showcase
      progress.
    </p>
    <p>
      <b>Start simple</b>, you don't need an automated way to gather the data.
      You can create a quick Google Form or simply ask your team to estimate how
      many PRs they merge each week, how much time they spend on new features
      compared with fixing bugs, or how they would grade the development
      experience.
    </p>
  </body>
</html>
